"""
LangChainì„ ì‚¬ìš©í•œ ë³´í—˜ ì¸ì •ê¸°ì¤€ RAG ì‹œìŠ¤í…œ
AWS Bedrock Claude 4.5 Haiku + í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
LCEL (LangChain Expression Language) ì‚¬ìš©
"""

import os
from typing import Dict, Any, List, Optional
from dotenv import load_dotenv

# LangChain ì„í¬íŠ¸ (ìµœì‹  LCEL ë°©ì‹)
from langchain_aws import ChatBedrock
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# ì»¤ìŠ¤í…€ ê²€ìƒ‰ê¸°
from tools.langchain_retriever import HybridLangChainRetriever

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class InsuranceLangChainAgent:
    """LangChain ê¸°ë°˜ ë³´í—˜ ì¸ì •ê¸°ì¤€ ë‹µë³€ ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        """ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        self.aws_region = os.getenv("AWS_REGION", "us-east-1")
        self.model_id = os.getenv(
            "BEDROCK_MODEL_ID",
            "anthropic.claude-4-5-haiku-20251015-v1:0"
        )
        
        # Claude ëª¨ë¸ ì´ˆê¸°í™”
        self.llm = ChatBedrock(
            model_id=self.model_id,
            region_name=self.aws_region,
            model_kwargs={
                "temperature": 0.1,  # ì¼ê´€ëœ ë‹µë³€
                "top_p": 0.9,
                "max_tokens": 2500
            }
        )
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
        self.retriever = HybridLangChainRetriever()
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        self.system_prompt = """ë‹¹ì‹ ì€ ê±´ê°•ë³´í—˜ì‹¬ì‚¬í‰ê°€ì›ì˜ ë³´í—˜ ì¸ì •ê¸°ì¤€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì—­í• :
- ì˜ë£Œê¸°ê´€ì˜ ë³´í—˜ì¬ë£Œ ë° ì‹œìˆ í–‰ìœ„ ì½”ë“œì— ëŒ€í•œ ê¸‰ì—¬ ì¸ì • ì—¬ë¶€ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.
- ìˆ˜ê°€ ì‚°ì • ë°©ë²•, ì²­êµ¬ ë°©ë²•, ì½”ë“œ ì¡°í•© ë“±ì„ ì•ˆë‚´í•©ë‹ˆë‹¤.
- ì‹¬í‰ì›ì˜ ì¸ì •ê¸°ì¤€ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ê·¼ê±°ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.
- ì‚­ê° ê°€ëŠ¥ì„±ì´ ìˆëŠ” ê²½ìš° ëª…í™•í•œ ì´ìœ ì™€ ê´€ë ¨ ë²•ë ¹ì„ ì œì‹œí•©ë‹ˆë‹¤.

ë‹µë³€ ì›ì¹™:
1. ì œê³µëœ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¼¼ê¼¼íˆ ì½ê³  ë¶„ì„í•©ë‹ˆë‹¤.
2. ê° ë¬¸ì„œì—ì„œ ë°œê²¬í•œ êµ¬ì²´ì ì¸ ì˜ˆì‹œ, ê·œì •, ê¸°ì¤€ì„ ëª…í™•íˆ í™•ì¸í•©ë‹ˆë‹¤.
3. êµ¬ì²´ì ì¸ ì½”ë“œ, ìˆ˜ê°€, ì‚°ì •ë°©ë²•ì„ ëª…ì‹œí•©ë‹ˆë‹¤.
4. ì¶”ì¸¡ì´ ì•„ë‹Œ, ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.

ë‹µë³€ í˜•ì‹:

**ğŸ“‹ ë¬¸ì„œ ë¶„ì„**:
- ë¬¸ì„œ 1: [í•µì‹¬ ê·œì •]
- ë¬¸ì„œ 2: [í•µì‹¬ ê·œì •]
- ì ìš© ê·œì •: [ê´€ë ¨ ê³ ì‹œ, ë²•ë ¹]

---

**ìˆ˜ê°€ ì‚°ì • ë°©ë²•** (í•´ë‹¹ ì‹œ):
1. ì£¼ ì‹œìˆ : ì½”ë“œ, ë‚´ì—­
2. ì¶”ê°€ ì‹œìˆ : ì½”ë“œ, ë‚´ì—­
3. ì¬ë£ŒëŒ€: ì‚°ì • ë°©ë²•

**ì²­êµ¬ ì˜ˆì‹œ**:
| ì½”ë“œ | í•­ëª© | ìˆ˜ëŸ‰ | ë‚´ì—­ |
|------|------|------|------|
| ì½”ë“œ | 08 | 1 | ì‹œìˆ ëª… |

**ì°¸ê³ ì‚¬í•­**:
- ì¶”ê°€ ì„¤ëª…

---

**íŒë‹¨** (ì‚­ê° ì§ˆë¬¸ ì‹œ): ì¸ì •ë¨/ì‚­ê° ê°€ëŠ¥ì„± ìˆìŒ/ì‚­ê°ë¨

**ê·¼ê±°**:
- ì¸ì •ê¸°ì¤€ì´ë‚˜ ì œì™¸ì‚¬í•­

**ê´€ë ¨ ë²•ë ¹**:
- ê³ ì‹œëª… ë° ì¡°í•­"""

        self.prompt_template = ChatPromptTemplate.from_messages([
            ("system", self.system_prompt),
            ("human", """ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ:

{context}

ì§ˆë¬¸: {question}

ìœ„ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.""")
        ])
        
        # ê°„ë‹¨í•œ ì²´ì¸ ìƒì„±
        self.chain = self.prompt_template | self.llm | StrOutputParser()
        
        print("âœ… LangChain Agent ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _format_docs(self, docs):
        """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í¬ë§·íŒ…"""
        if not docs:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        formatted = []
        for i, doc in enumerate(docs, 1):
            metadata = doc.metadata
            formatted.append(
                f"[ë¬¸ì„œ {i}] {metadata.get('filename', 'Unknown')} "
                f"(ì ìˆ˜: {metadata.get('score', 0):.4f})\n"
                f"{doc.page_content}\n"
            )
        
        return "\n".join(formatted)
    
    def answer_query(
        self,
        question: str,
        material_code: Optional[str] = None,
        procedure_code: Optional[str] = None,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        ì§ˆì˜ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            material_code: ì¬ë£Œì½”ë“œ (ì„ íƒì‚¬í•­)
            procedure_code: ì‹œìˆ ì½”ë“œ (ì„ íƒì‚¬í•­)
            conversation_history: ì´ì „ ëŒ€í™” ë‚´ì—­
            
        Returns:
            ë‹µë³€ ë”•ì…”ë„ˆë¦¬
        """
        try:
            # ì§ˆë¬¸ì— ì½”ë“œ ì •ë³´ ì¶”ê°€
            full_question = question
            if material_code:
                full_question = f"ì¬ë£Œì½”ë“œ: {material_code}\n{full_question}"
            if procedure_code:
                full_question = f"ì‹œìˆ ì½”ë“œ: {procedure_code}\n{full_question}"
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ê°„ë‹¨í•œ ë²„ì „)
            if conversation_history and len(conversation_history) > 0:
                history_text = "\nì´ì „ ëŒ€í™”:\n"
                for msg in conversation_history[-2:]:  # ë§ˆì§€ë§‰ 2ê°œë§Œ
                    role = "ì‚¬ìš©ì" if msg["role"] == "user" else "AI"
                    history_text += f"{role}: {msg['content'][:100]}...\n"
                full_question = history_text + "\n" + full_question
            
            # 1. ë¬¸ì„œ ê²€ìƒ‰
            docs = self.retriever._get_relevant_documents(full_question, k=5)
            
            if not docs:
                return {
                    "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.",
                    "sources": [],
                    "material_code": material_code,
                    "procedure_code": procedure_code,
                    "question": question
                }
            
            # 2. ì²´ì¸ ì‹¤í–‰
            context_text = self._format_docs(docs)
            answer = self.chain.invoke({
                "context": context_text,
                "question": full_question
            })
            
            # 3. ì†ŒìŠ¤ ë¬¸ì„œ ì¶”ì¶œ
            sources = []
            for doc in docs:
                metadata = doc.metadata
                sources.append({
                    "type": metadata.get("file_type", "document"),
                    "ì¬ë£Œì½”ë“œ": metadata.get("ì¬ë£Œì½”ë“œ"),
                    "ì¬ë£Œëª…": metadata.get("ì¬ë£Œëª…"),
                    "ì‹œìˆ ì½”ë“œ": metadata.get("ì‹œìˆ ì½”ë“œ"),
                    "ì‹œìˆ ëª…": metadata.get("ì‹œìˆ ëª…"),
                    "score": metadata.get("score", 0)
                })
            
            return {
                "answer": answer,
                "sources": sources,
                "material_code": material_code,
                "procedure_code": procedure_code,
                "question": question
            }
            
        except Exception as e:
            error_msg = f"LangChain Agent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            
            return {
                "answer": error_msg,
                "sources": [],
                "material_code": material_code,
                "procedure_code": procedure_code,
                "question": question,
                "error": str(e)
            }


# ì „ì²´ íŒŒì´í”„ë¼ì¸ (LangChain ì‚¬ìš©)
def answer_insurance_query_langchain(
    question: str,
    material_code: Optional[str] = None,
    procedure_code: Optional[str] = None,
    conversation_history: Optional[List[Dict[str, str]]] = None
) -> Dict[str, Any]:
    """
    ë³´í—˜ ì¸ì •ê¸°ì¤€ ì§ˆì˜ íŒŒì´í”„ë¼ì¸ (LangChain)
    
    Args:
        question: ì§ˆë¬¸
        material_code: ì¬ë£Œì½”ë“œ (ì„ íƒì‚¬í•­)
        procedure_code: ì‹œìˆ ì½”ë“œ (ì„ íƒì‚¬í•­)
        conversation_history: ì´ì „ ëŒ€í™” ë‚´ì—­ (ì„ íƒì‚¬í•­)
        
    Returns:
        ë‹µë³€ ê²°ê³¼
    """
    agent = InsuranceLangChainAgent()
    return agent.answer_query(
        question=question,
        material_code=material_code,
        procedure_code=procedure_code,
        conversation_history=conversation_history
    )


# í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜
if __name__ == "__main__":
    print("=" * 60)
    print("LangChain ë³´í—˜ ì¸ì •ê¸°ì¤€ ë‹µë³€ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ ì§ˆì˜
    result = answer_insurance_query_langchain(
        question="RCAì™€ LADì— ìŠ¤í…íŠ¸ë¥¼ ì‚½ì…í•œ ê²½ìš° ìˆ˜ê°€ ì‚°ì •ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
    )
    
    print("\nì§ˆë¬¸:", result['question'])
    print("\në‹µë³€:")
    print(result['answer'])
    print("\nì°¸ê³  ë¬¸ì„œ:", len(result['sources']), "ê°œ")
    for i, source in enumerate(result['sources'], 1):
        print(f"  [{i}] {source['type']} (ì ìˆ˜: {source.get('score', 0):.4f})")

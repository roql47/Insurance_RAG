"""
ì¦ë¶„ í•™ìŠµ: ê¸°ì¡´ í•™ìŠµ ë°ì´í„°ë¥¼ ìœ ì§€í•˜ê³  ìƒˆë¡œìš´ PDFë§Œ ì¶”ê°€ë¡œ í•™ìŠµ
"""

import json
import os
import pickle
import glob
from typing import List, Dict, Any, Set
import numpy as np
import faiss
from dotenv import load_dotenv
from datetime import datetime

from tools.embedder_tool import TitanEmbedder
from tools.document_loader import DocumentLoader

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class IncrementalPDFPreprocessor:
    """ì¦ë¶„ í•™ìŠµì„ ì§€ì›í•˜ëŠ” PDF ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.embedder = TitanEmbedder()
        self.loader = DocumentLoader()
        self.vector_store_path = os.getenv("VECTOR_STORE_PATH", "./data/vector_store")
        self.processed_files_path = os.path.join(self.vector_store_path, "processed_files.json")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.vector_store_path, exist_ok=True)
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ
        self.index = None
        self.metadata = []
        self.processed_files = set()
        self._load_existing_data()
    
    def _load_existing_data(self):
        """ê¸°ì¡´ FAISS ì¸ë±ìŠ¤, ë©”íƒ€ë°ì´í„°, ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ë¡œë“œ"""
        index_path = os.path.join(self.vector_store_path, "faiss_index.bin")
        metadata_path = os.path.join(self.vector_store_path, "metadata.pkl")
        
        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        if os.path.exists(index_path):
            self.index = faiss.read_index(index_path)
            print(f"âœ… ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ: {self.index.ntotal}ê°œ ë²¡í„°")
        else:
            print("â„¹ï¸  ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ì—†ìŒ (ìƒˆë¡œ ìƒì„±)")
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        if os.path.exists(metadata_path):
            with open(metadata_path, 'rb') as f:
                self.metadata = pickle.load(f)
            print(f"âœ… ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë¡œë“œ: {len(self.metadata)}ê°œ í•­ëª©")
        else:
            print("â„¹ï¸  ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì—†ìŒ (ìƒˆë¡œ ìƒì„±)")
        
        # ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ë¡œë“œ
        if os.path.exists(self.processed_files_path):
            with open(self.processed_files_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.processed_files = set(data.get('files', []))
            print(f"âœ… ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ë¡œë“œ: {len(self.processed_files)}ê°œ íŒŒì¼")
            for filename in self.processed_files:
                print(f"   - {filename}")
        else:
            print("â„¹ï¸  ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì—†ìŒ")
    
    def _save_processed_files(self):
        """ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì €ì¥"""
        data = {
            'files': list(self.processed_files),
            'last_updated': datetime.now().isoformat()
        }
        with open(self.processed_files_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def process_new_pdfs(self, pdf_folder: str, force_reprocess: bool = False):
        """
        ìƒˆë¡œìš´ PDF íŒŒì¼ë§Œ ì²˜ë¦¬í•˜ì—¬ ê¸°ì¡´ ì¸ë±ìŠ¤ì— ì¶”ê°€
        
        Args:
            pdf_folder: PDF íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë” ê²½ë¡œ
            force_reprocess: Trueë©´ ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ë„ ë‹¤ì‹œ ì²˜ë¦¬
        """
        print("=" * 60)
        print("ì¦ë¶„ í•™ìŠµ: ìƒˆë¡œìš´ PDFë§Œ ì¶”ê°€")
        print("=" * 60)
        
        # PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        pdf_pattern = os.path.join(pdf_folder, "*.pdf")
        all_pdf_files = glob.glob(pdf_pattern)
        
        if not all_pdf_files:
            print(f"âŒ ì˜¤ë¥˜: {pdf_folder}ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nì „ì²´ PDF íŒŒì¼: {len(all_pdf_files)}ê°œ")
        
        # ìƒˆë¡œìš´ íŒŒì¼ë§Œ í•„í„°ë§
        if force_reprocess:
            new_pdf_files = all_pdf_files
            print("âš ï¸  ê°•ì œ ì¬ì²˜ë¦¬ ëª¨ë“œ: ëª¨ë“  íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        else:
            new_pdf_files = [
                pdf for pdf in all_pdf_files 
                if os.path.basename(pdf) not in self.processed_files
            ]
        
        if not new_pdf_files:
            print("\nâœ… ì²˜ë¦¬í•  ìƒˆë¡œìš´ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ†• ìƒˆë¡œ ì²˜ë¦¬í•  íŒŒì¼: {len(new_pdf_files)}ê°œ")
        for i, pdf_file in enumerate(new_pdf_files, 1):
            print(f"  [{i}] {os.path.basename(pdf_file)}")
        print()
        
        # ìƒˆë¡œìš´ ì²­í¬ ìˆ˜ì§‘
        new_chunks = []
        successfully_processed = []
        
        # ê° ìƒˆ PDF íŒŒì¼ ì²˜ë¦¬
        for i, pdf_path in enumerate(new_pdf_files, 1):
            filename = os.path.basename(pdf_path)
            print(f"\n[{i}/{len(new_pdf_files)}] ì²˜ë¦¬ ì¤‘: {filename}")
            print("-" * 60)
            
            try:
                # PDF ë¡œë“œ
                doc_data = self.loader.load_pdf(pdf_path, method="pdfplumber")
                
                # ì²­í¬ ìƒì„±
                chunks = self.loader.chunk_document(
                    doc_data,
                    chunk_size=1000,
                    overlap=100
                )
                
                # ë©”íƒ€ë°ì´í„° ê°•í™”
                for chunk in chunks:
                    chunk['metadata']['document_title'] = doc_data.get('filename', 'Unknown')
                    chunk['metadata']['source_type'] = 'pdf'
                    chunk['metadata']['source_file'] = filename
                    chunk['metadata']['processed_date'] = datetime.now().isoformat()
                
                new_chunks.extend(chunks)
                successfully_processed.append(filename)
                print(f"[OK] ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
                
            except Exception as e:
                print(f"[ERROR] ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                print(f"  íŒŒì¼ ê±´ë„ˆë›°ê¸°: {filename}")
                continue
        
        if not new_chunks:
            print("\nâŒ ì²˜ë¦¬ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\n" + "=" * 60)
        print(f"ìƒˆë¡œ ìƒì„±ëœ ì²­í¬: {len(new_chunks)}ê°œ")
        print("=" * 60)
        
        # ì„ë² ë”© ìƒì„±
        print("\nì„ë² ë”© ìƒì„± ì‹œì‘...")
        texts = [chunk['text'] for chunk in new_chunks]
        embeddings = self.embedder.embed_texts(texts)
        
        for i, chunk in enumerate(new_chunks):
            chunk['embedding'] = embeddings[i]
        
        print("âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ì— ì¶”ê°€
        self.add_to_faiss(new_chunks)
        
        # ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì—…ë°ì´íŠ¸
        self.processed_files.update(successfully_processed)
        self._save_processed_files()
        
        print("\n" + "=" * 60)
        print("ì¦ë¶„ í•™ìŠµ ì™„ë£Œ")
        print("=" * 60)
        print(f"\n[SUCCESS] {len(successfully_processed)}ê°œ PDF íŒŒì¼, {len(new_chunks)}ê°œ ì²­í¬ ì¶”ê°€ ì™„ë£Œ!")
        print(f"ì´ ë²¡í„° ìˆ˜: {self.index.ntotal}ê°œ")
    
    def add_to_faiss(self, new_chunks: List[Dict[str, Any]]):
        """
        ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ì— ìƒˆë¡œìš´ ì²­í¬ ì¶”ê°€
        
        Args:
            new_chunks: ì„ë² ë”©ì´ í¬í•¨ëœ ìƒˆ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        print("\nFAISS ì¸ë±ìŠ¤ì— ì¶”ê°€ ì¤‘...")
        
        # ì„ë² ë”© ì¶”ì¶œ
        valid_chunks = [c for c in new_chunks if c['embedding'] is not None]
        new_embeddings = np.array([c['embedding'] for c in valid_chunks], dtype='float32')
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if self.index is None:
            dimension = new_embeddings.shape[1]
            self.index = faiss.IndexFlatL2(dimension)
            print(f"ìƒˆ FAISS ì¸ë±ìŠ¤ ìƒì„± (ì°¨ì›: {dimension})")
        
        # ë²¡í„° ì¶”ê°€
        self.index.add(new_embeddings)
        print(f"âœ… {len(valid_chunks)}ê°œ ë²¡í„° ì¶”ê°€ë¨ (ì´ {self.index.ntotal}ê°œ)")
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for chunk in valid_chunks:
            metadata = {
                "text": chunk['text'],
                "metadata": chunk['metadata']
            }
            self.metadata.append(metadata)
        
        # ì €ì¥
        self._save_index_and_metadata()
    
    def _save_index_and_metadata(self):
        """FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        # ì¸ë±ìŠ¤ ì €ì¥
        index_path = os.path.join(self.vector_store_path, "faiss_index.bin")
        faiss.write_index(self.index, index_path)
        print(f"âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥: {index_path}")
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_path = os.path.join(self.vector_store_path, "metadata.pkl")
        with open(metadata_path, 'wb') as f:
            pickle.dump(self.metadata, f)
        print(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
    
    def reset_processed_files(self):
        """ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì´ˆê¸°í™” (ì „ì²´ ì¬í•™ìŠµìš©)"""
        if os.path.exists(self.processed_files_path):
            os.remove(self.processed_files_path)
        self.processed_files = set()
        print("âœ… ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì´ˆê¸°í™”ë¨")
    
    def list_processed_files(self):
        """ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì¶œë ¥"""
        if not self.processed_files:
            print("ì²˜ë¦¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ({len(self.processed_files)}ê°œ):")
        for i, filename in enumerate(sorted(self.processed_files), 1):
            print(f"  [{i}] {filename}")


# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    import sys
    import argparse
    
    parser = argparse.ArgumentParser(description="ì¦ë¶„ í•™ìŠµ: ìƒˆë¡œìš´ PDFë§Œ ì¶”ê°€")
    parser.add_argument("pdf_folder", help="PDF íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë” ê²½ë¡œ")
    parser.add_argument("--force", action="store_true", help="ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ë„ ë‹¤ì‹œ ì²˜ë¦¬")
    parser.add_argument("--reset", action="store_true", help="ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì´ˆê¸°í™”")
    parser.add_argument("--list", action="store_true", help="ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì¶œë ¥")
    
    args = parser.parse_args()
    
    # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    preprocessor = IncrementalPDFPreprocessor()
    
    # ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì¶œë ¥
    if args.list:
        preprocessor.list_processed_files()
        sys.exit(0)
    
    # ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ ì´ˆê¸°í™”
    if args.reset:
        confirm = input("âš ï¸  ì²˜ë¦¬ëœ íŒŒì¼ ëª©ë¡ì„ ì´ˆê¸°í™”í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if confirm.lower() == 'y':
            preprocessor.reset_processed_files()
        sys.exit(0)
    
    # PDF í´ë” í™•ì¸
    if not os.path.exists(args.pdf_folder):
        print(f"âŒ ì˜¤ë¥˜: í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.pdf_folder}")
        sys.exit(1)
    
    # ì¦ë¶„ í•™ìŠµ ì‹¤í–‰
    try:
        preprocessor.process_new_pdfs(args.pdf_folder, force_reprocess=args.force)
        print("\nâœ… ì„±ê³µ! ì´ì œ 'python run_server.py'ë¡œ API ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

